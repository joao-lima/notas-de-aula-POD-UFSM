%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Introdução a análise e complexidade de algoritmos}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Definição de algoritmo}

\begin{itemize}
\item Procedimento bem definido que:
	\begin{itemize}
	\item recebe como entrada um valor, ou conjunto de valores
	\item produz como saída um valor ou conjunto de valores
	\end{itemize}
\item Conjunto de passos que transforma a entrada na saída.
\end{itemize}

Exemplo do {\bf problema de ordenação}:
\begin{itemize}
\item {\bf Entrada}: uma sequência de $n$ números $\langle a_1, a_2, ..., a_n \rangle$.

\item {\bf Saída}:  Uma permutação $\langle {a'}_1, {a'}_2, ..., {a'}_n \rangle$ da
entrada tal que ${a'}_1 \leq {a'}_2 \leq ... \leq {a'}_n \rangle$.
\end{itemize}

\begin{itemize}
\item Essa entrada do algoritmo é uma {\bf instância} do problema.
\item Para um algoritmo estar {\bf correto}, para cada instância, ele termina com saída 
correta. Um algoritmo correto \textit{resolve} um problema. 
\end{itemize}

Exemplos de problemas:
\begin{itemize}
\item O Projeto do Genoma Humano para identificar todos os genes do DNA humano.
\item Internet com descoberta de rotas.
\item E-commerce.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Análise de algoritmos}

\begin{itemize}
\item Medida de custo de execução de um programa.
\item Importante separar entre uma medida {\bf real}  e {\bf generalizada}.
	\begin{itemize}
	\item real depende do compilador, hardware, memória, etc.
	\item generalizada pode usar um computador abstrato para análise.
		\begin{itemize}
		\item Ex: máquina teóricas (Turing, lambda).
		\end{itemize}
	\end{itemize}
\item Grande parte dos casos utiliza-se o número de {\bf elementos de entrada}, denotado $n$, para
determinar o tempo de um algoritmo.
\item Se fosse um grafo $G = (V, E)$, poderia-se usar na análise $|V|$, ou seja, o número de vértices ou
o número de arestas $|E|$.
\item O {\bf tempo de execução} é o número de passos executados, sendo o mais independente de 
máquina possível.
\end{itemize}

Vamos considerar dois exemplos de algoritmos de ordenação com $n$ elementos de entrada:
\begin{itemize}
\item {\bf Ordenação por inserção} com custo $c_1 n^2$ sendo $c_1$ uma constante que não depende de $n$.
\item {\bf Ordenação merge sort} com custo $c_2 n \log n$ sendo $c_2$ outra constante que não depende de $n$.
\end{itemize}

Considere um exemplo onde vamos executar a ordenação por inserção em uma
máquina mais rápida (A) e a ordenação por mergesort em uma máquina mais lenta
(B).
A entrada será de 10 milhões de números ($10^7$ ou equivalente a $80$ MB).
Suponha que a máquina A executa $10^{10}$ instruções por segundo, enquanto que a máquina B executa
somente $10^7$ instruções por segundo. 

Ainda, dadas as implementações dos algoritmos, o código da inserção tem custo
$2n^2$ enquanto que o mergesort tem $50n \log n$ de custo.

Para ordenar $10^7$ números, o computador A com ordenação por inserção demora
\begin{equation*}
\frac{2 \dot (10^7)^2\; instrucoes}{10^{10}\; instrucoes/segundo} = 20.000 \; segundos.
\end{equation*}

Depois, para ordenar os $10^7$ números no computador B com mergesort custa
\begin{equation*}
\frac{50 \dot (10^7) \log 10^7\; instrucoes}{10^{7}\; instrucoes/segundo} = 1.163 \; segundos \;(20\; minutos).
\end{equation*}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Função de complexidade}

O custo de um algoritmo é definido em geral por uma função de complexidade $f$. Sendo assim,
$f(n)$ é a medida de tempo de execução de um algoritmo para uma instância do problema de tamanho $n$.
Podemos ter uma função $f(n)$ que descreve:
\begin{itemize}
\item Complexidade de {\bf tempo} onde $f(n)$ mede o tempo para executar um problema $n$.
\item Complexidade de {\bf espaço} onde $f(n)$ mede o espaço de memória ocupado pelo algoritmo
com entrada $n$.
\end{itemize}

Caso nada for mencionado, estamos falando sobre complexidade de tempo em grande parte do material.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analise simples - maior elemento}

A figura~\ref{aula02:algo:max} mostra o algoritmo que encontra o maior elemento em um vetor.
Seja $f(n)$ a função de complexidade que determina o número de comparações entre elementos de $A$.
Logo, $f(n) = n - 1$, para $n > 0$.
%
\begin{figure}[!htb]
\centering
\begin{framed}
\begin{lstlisting}
int Max( int *A, int n ){
	int i, temp;
	temp = A[0];
	for( i = 1; i < n; i++ ){
		if( temp < A[i] )
			temp = A[i];
	}
	return temp;
}
\end{lstlisting}
\end{framed}
\caption{Algoritmo que encontra o maior elemento.}
\label{aula02:algo:max}
\end{figure}

No caso do algoritmo de maior elemento, o custo de execução é uniforme sobre todos os problemas 
de tamanho $n$.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Analise da ordenação por inserção}

O código da figura~\ref{aula02:algo:insertion} mostra o algoritmo da ordenação
por inserção, enquanto a figura~\ref{aula02:fig:insertion} apresenta os passos
na ordenação de 6 números.
%
\begin{figure}[!htb]
\centering
\begin{framed}
\begin{lstlisting}
void Insercao( int *A, int n ) {
	int i, j;
	int x;
	for( j = 2; j <= n; j++ ){
		x = A[j];
		i = j - 1;
		while( i > 0 && x  < A[i] ){
			A[i+1] = A[i];
			i--;
		}
		A[i+1] = x;
	}
}
\end{lstlisting}
\end{framed}
\caption{Algoritmo de ordenação por inserção.}
\label{aula02:algo:insertion}
\end{figure}
%
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{02-insertionsort}
\caption{Passos da ordenação por inserção.}
\label{aula02:fig:insertion}
\end{figure}

A ordem dos elementos na ordenação por inserção depende do tamanho da entrada $n$. 
Além disso, o tempo de execução pode variar conforme a ordem dos elementos.
%
O tempo de execução depende da soma dos tempos de execução de cada passo do algoritmo
executado: se o passo tem custo $c_i$ e executa $n$ vezes contribui $c_i n$ para o tempo
total.
Assumindo o pior caso, quando a entrada está em ordem reversa, o custo de execução é 
$an^2 + bn +c$ para constantes $a$, $b$ e $c$.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Melhor caso, caso médio e pior caso}

Em alguns algoritmos, o custo de execução também é uma função da entrada de dados, não somente
do tamanho $n$ da entrada.
No exemplo anterior, demostramos apenas o pior caso. 
Os possíveis casos para análise são:
\begin{itemize}
\item {\bf Pior caso} -- quando os elementos estão em ordem inversa e todos os elementos 
são trocados.
Sabendo o pior caso, o custo do algoritmo nunca é maior que $f(n)$.
O custo seria $an^2 + bn +c$ para constantes $a$, $b$ e $c$.

\item {\bf Melhor caso} -- onde os elementos já estão ordenados e nenhuma troca é necessária.
O custo é $an + b$ para constantes $a$ e $b$.

\item {\bf Caso médio} -- pode ser tanto quanto o pior caso. Supondo que aplicamos 
a ordenação por inserção para elementos em ordem aleatória $n$. 
O resultado de uma execução de caso médio será quadratica ($an^2 + bn +c$)
tal como no pior caso.
\end{itemize}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Ordem de crescimento}

A {\bf ordem de crescimento} é uma forma de simplificar a analise de algoritmos
e desconsiderar detalhes de custo.
Na ordernação por inserção, consideramos apenas o termo maior ($an^2$) e desconsideramos
sua constante ($a$) e os termos menores ($n$). 
Para grandes valores de $n$, os termos menores são relativamente insignificantes.

Na tabela abaixo, é possível comparar as diferentes funções de crescimento e 
o tempo de execução de acordo com a entrada.
\begin{table}[ht]
\centering
\begin{tabular}{cccccc}
\hline
{\bf descrição} & {\bf função} & {\bf 2x} & {\bf 10x} & {\bf tempo$10n$} & {\bf tempo $10n$ ($10\times$ rápido)} \\ 
\hline
constante        & $1$ & & & &  \\
linear           & $n$        & $2$ & $10$ & um dia & algumas horas \\
linear-logaritmo & $n \log n$ & $2$ & $10$ & um dia & algumas horas \\
quadratico       & $n^2$      & $4$  & $100$ & algumas semanas & um dia  \\
cúbico           & $n^3$ & $8$ & $1.000$ & meses & algumas semanas  \\
exponencial      & $2^n$  & $2^n$ & $2^{9n}$ & nunca & nunca  \\
\hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Notação assintótica}

A notação assintótica é uma forma de denotar o crescimento de uma função,
e uma notação para comparar algoritmos.
%
\begin{figure}[ht]
\centering
\includegraphics[width=0.8\textwidth]{02-assintotico}
\caption{Comparação das notações $\Theta$, $O$ e $\Omega$.}
\label{aula02:fig:assintotico}
\end{figure}

\begin{itemize}
\item $O$ (\emph{big-oh}) define o {\bf limite assintótico superior}, ou seja, 
limite para o tempo de execução para o pior caso. Escrevemos $g(n) = O(f(n))$ para
expressar que $f(n)$ domina assintoticamente $g(n)$. Para
uma função $g(n)$ define-se $O(g(n))$ o conjunto de funções 
$O(g(n)) = \{f(n):$ existe constantes positivas $c$ e $n_0$ tal que $0 \leq f(n) \leq c g(n)$ 
para todo $n \geq n_0\}$.

\item $\Theta$ é um {\bf limite assintótico firme} onde 
para uma função $g(n)$ define-se $\Theta(g(n))$ o conjunto de funções 
$\Theta(g(n)) = \{f(n):$ existe constantes positivas $c_1$, $c_2$ e $n_0$ tal que 
$0 \leq c_1 g(n) \leq f(n) \leq c_2 g(n)$ 
para todo $n \geq n_0\}$.

\item $\Omega$ é um {\bf limite inferior} onde para uma função $g(n)$ 
define-se $\Omega(g(n))$ o cojunto de funções
$O(g(n)) = \{f(n):$ existe constantes positivas $c$ e $n_0$ tal que $0 \leq c g(n) \leq f(n)$ 
para todo $n \geq n_0\}$.

\end{itemize}

No caso da ordenação por inserção, podemos afirmar que o tempo de execução fica entre
$\Omega(n)$ (melhor caso) e $O(n^2)$ (pior caso).


Nesse curso vamos focar na notação $O$, ou seja, onde $f(n) = O(g(n))$ tal que
$f(n) \leq cg(n)$ para $n \geq n_0$.
Exemplos:
\begin{itemize}
\item $2n + 10$ é $O(n)$:
	\begin{itemize}
	\item $2n + 10 \leq cn$
	\item $(c - 2)n \geq 10$
	\item $n \geq 10/(c - 2)$
	\item Vale para $c = 3$ e $n_0 = 10$.
	\end{itemize}
\item $n^2$ não é $O(n)$:
	\begin{itemize}
	\item $n^2 \leq cn$
	\item $n \leq c$
	\item Não pode ser satisfeito porque $c$ precisa ser constante.
	\end{itemize}
\item $7n - 2$ é $O(n)$.
\item $3n^3 + 20n^2 + 5$ é $O(n^3)$.
\item $3 \log (n) + 5$ é $O(\log (n))$.
\end{itemize}

Alguma regras para a notação $O$:
\begin{itemize}
\item Se $f(n)$ tem grau $d$ então $f(n)$ é $O(n^d)$, ou seja:
	\begin{itemize}
	\item Ignora items de ordem menor.
	\item Ignora constantes.
	\end{itemize}
\item Use a menor ordem possível.
	\begin{itemize}
	\item ``$2n$ é $O(n)$'' invés de ``$2n$ é $O(n^2)$''.
	\end{itemize}
\item Use a expressão mais simples da ordem.
	\begin{itemize}
	\item ``$3n + 5$ é $O(n)$'' invés de ``$3n+5$ é $O(3n)$''.
	\end{itemize}
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Principais classes de problemas}

\begin{itemize}
\item $f(n) = O(1)$ são algoritmos de {\bf complexidade constante}
independentes de $n$ com número de instruções fixas.

\item $f(n) = O(\log n)$ são algoritmos de {\bf complexidade logarítmica}.
Classe típica de problemas que dividem um problema em outros menores (divisão e conquista).

\item $f(n) = O(n)$ são algoritmos de {\bf complexidade linear}. Em geral 
um pequeno trabalho é realizado sobre cada elemento.

\item $f(n) = O(n \log n)$ são algoritmos que em geral quebram um problema maior em outros
menores, resolvem cada um deles e juntam as soluções depois.

\item $f(n) = O(n^2)$ são algoritmos de {\bf complexidade quadrática}. 
Ocorrem quando os dados são processados aos pares, muitas vezes em laços aninhados.

\item $f(n) = O(n^3)$ são algoritmos de \textbf{complexidade cúbica}. Um exemplo
desses algoritmos é o caso de três laços aninhados.

\item $f(n) = O(2^n)$ são algoritmos de \textbf{complexidade exponencial}. Eles
não são úteis do ponto de vista prático (problemas \textbf{NP-complete}.
Em geral ocorrem em soluções que usam \textit{força bruta}. Exemplo: quando $n$ é 20,
o tempo de execução chega a $1$ milhão.

\item $f(n) = O(n!)$ são algoritmos de complexidade exponencial, apesar
de $O(n!)$ ser muito pior que $O(2^n)$. Também ocorrem em casos de força bruta.
\end{itemize}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Algoritmos polinomiais e exponenciais}

\begin{itemize}
\item {\bf Algoritmo polinomial} em tempo de execução tem complexidade $O(c^n), c > 1$.
\item {\bf Algoritmo exponencial} em tempo de execução tem complexidade $O(p(n))$, onde
$p(n)$ é um polinômio.
\end{itemize}


\begin{framed}
\centering
Um algoritmo é eficiente quando seu tempo de execução é polinomial.
\end{framed}

Um problema é dito \textbf{intratável} se não existe um algoritmo polinomial para resolvê-lo.
